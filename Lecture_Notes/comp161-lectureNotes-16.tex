\documentclass[]{tufte-handout}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
  frame=tb,
  language=C++,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  stepnumber=1,    
  firstnumber=1,
  numbersep=5pt,
  numberfirstline=true,
  numberstyle=\tiny\color{black},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}



\title{COMP 161 - Lecture Notes 16 - Analyzing Search and Sort}
\date{}

\begin{document} 
\maketitle

\begin{abstract}
In these notes we analyze search and sort. 
\end{abstract}

\section{Counting Operations}

When we analyze the complexity of procedures we're determine the order of the number of \textbf{primitive operations} performed in the worst case of the procedure. Primitive operations are the operations we call on primitive data types. We count one such operation as one unit of work\sidenote{The reality is that they are $O(1)$ hardware operations, but we ignore that because we're drawing the line at primitive C++}. Almost all the operators we've encountered are primitive, but in C++ programmers can provide new definitions for operators so we must be careful about the assumption that all operators are primitive. 

\section{Search is Linear Time}

So how good is our search and is there a difference between the iterative and recursive implementations. To answer this we first turn to complexity analysis to see how each implementation is classified. 

\subsection{Iterative Search}

Let's start by re-introducing ourselves to the search code shown in figure \ref{code:searchiter}.
\begin{figure}[htpb!]
\begin{lstlisting}
int search(const std::vector<int>& data,int fst, int lst, int key){

    for(unsigned int i{0}; i < data.size() ; ++i){
    	if( data[i] == key ){
			return i;
		}
    }

    return -1;
}
\end{lstlisting}
\label{code:searchiter}
\caption{Search: Iterative Implementation}
\end{figure}

The first question of complexity analysis is \textbf{what's the worst case scenario for this code?} By this we mean, for what inputs will we maximize the work done? Looking a the code we see that either the loop runs to completion and the function returns -1 or it will terminate early and return the current value of $i$. Computational work will, therefore, be maximized when the loop completes and this occurs when the key is not in the vector. From here we can see that when the key is in the vector, then we'll do less work the closer the key is to the $0$ location. 

Now that we know the worst case occurs when the key isn't found, we can start counting up the work. This is where Big-O steps in. We'll count up work in pieces, determine the order of each piece, and then use the sum rule to reduce the total order of the procedure. All of the work done by this procedure is done by the loop, so this is really an example of loop analysis. To understand the work carried out by the loop we address three questions:
\begin{enumerate}
\item How much, if any, work is done but not repeated?
\item How many times does the loop repeat itself?
\item How much work is done per repetition?
\end{enumerate}
If a loop repeats itself $n$ times, does $k$ work per repetition and $c$ work outside of that, then the total work is $kn+c$. If $k$, $n$, and $c$ are all constants then we're looking at some constant amount of work. For example, exactly $5$ repetitions of $10$ operations with $3$ other operations on the side is just $O(53) = O(1)$ work. If, however, some of these values vary, then the loop is more complexity then constant. 

A \textit{for} loop has two parts that are done regardless of the number of repetitions: the initialization statement and one instance of the loop continuation check. The continuation check is the one check that must be false in order to terminate the loop. For \textit{search} this is the initialization of $i$ and one check that $i$ is less than the size of the vector \textit{data}. The initialization and the comparison itself are both primitive, so that's a constant number of operations\sidenote{$2 = O(1)$}. Determining the size of a vector requires a std::vector method.  A quick check of the std::vector reference tells us that the complexity of the size method is constant. Maybe it's 5 operations, maybe 500. Either way, the actual size of the vector doesn't influence the cost of looking up its size. All told, the operations not repeated by the loop itself but part of the loop structure are constant, or $O(1)$.

Now, let's address the work repeated by the loop before we figure out the number of operations.  Every time the \textit{for} loop repeats it will do the continuation check and the update.  We already know the continuation check is $O(1)$ operations. The update is just $i++$ which is one assignment and one addition\sidenote{$i=i+1$}, or just a constant, $O(1)$ number of operations. So far we know that the repeated code used by the loop itself is $O(1)$ operations per repetition. Now we look at the body of the loop. Every time the loop repeats the conditional check is done. This requires selecting the $i^{th}$ integer with the std::vector \textit{operator[]} and then comparing it to another integer with primitive integer $==$.  The comparison is a single operation and if we look up \textit{operator[]} in the vector reference we see it too is $O(1)$. Combining the $O(1)$ operation inside the loop with the $O(1)$ operations done to control the loop gives us $O(1)+O(1)=O(1 + 1)=O(2)=O(1)$ operations. 

Finally, we must determine how many times the loop repeats. If we start at $0$, count up by $1$, and stop after $i = data.size()$ then we'll do one iteration of the loop for every element of the vector. This was maybe obvious because that was our goal for the loop, to traverse and visit every single location in the vector. If $n$ is the size of the vector, then this loop repeats $n = O(n)$ times in the worst case scenario when the key is not contained in the vector. Each repetition requires $O(1)$ work for a total of $O(n)*O(1) = O(n)$ work. \textit{Iterative search has a complexity that is linear in the size of the vector we are searching.}

\subsection{Recursive search}

The recursive search is spread across two procedures as shown in figure \ref{code:searchrec}. 

\begin{figure}[htpb!]
\begin{lstlisting}
int search(const std::vector<int>& data,int key){
	return search(data,0,data.size(),key);
}

int search(const std::vector<int>& data,int fst, int lst, int key){
	if( fst >= lst ){
		return -1;
	}
	else if( data[fst] == key ){
		return fst;
	}
	else{
		return search(data,fst+1,lst,key);
	}
}
\end{lstlisting}
\label{code:searchrec}
\caption{Search: Recursive Implementation} 
\end{figure}

Analyzing the recursion itself requires a combination of knowing how to handle the use of helper procedures and knowing how a recursive process is carried out by the computer. The top level \textit{search} function makes a call to the more general, recursive search.  While we can't know the exact complexity of the process carried out by this search without knowing how the recursive helper behaves, we can fully understand the work done by the top-level search and not a part of the recursion. Before the recursive call is made we determine the size of the vector data, that's it. This means the two argument, top-level search does a constant, $O(1)$, amount of work \textit{plus} whatever the recursive call with will do given its parameters. That's it.  

In general, it's important to be able to say something definitive about a procedure's complexity in the face of uncertainty about helper complexity. As the search example demonstrates, it's simply a matter of identifying the work done outside the helper. We can actually go further with this search. If the recursive search is anything other than constant complexity, then we know that the work done by that recursion will dominate the $O(1)$ work done outside the recursion.\sidenote{the sum of two different classes is the max of the classes} If the recursion itself is constant, then the sum total is constant\sidenote{the basic sum rule of Big-O}. This means that the top level search complexity will be as complex as the recursive helper, no more no less.

Now we must manage the recursive version of search. Let's begin with the exact strategy we used for the top-level function and identify the complexity of all the work done outside of the recursive call. Under what circumstances is this work maximized? To answer this we need to look carefully at the conditional. The more conditions that fail, the more work done by the conditional because every test must be carried out.  So the conditional itself will do the maximum amount of work with the \textit{if} and \textit{else if} condition are false. In that case each comparison requires $O(1)$ work\sidenote{so in complexity terms best and worst are the same $O(1)$}. The body of the \textit{else} then requires another addition before the recursion, so we're still looking at some constant, $O(1)$, work. All told, each time search is called in the course of the recursion, it will perform $O(1)$ work.  Figuring this out leaves us with one question: how many times does this procedure call itself?\sidenote{notice the overall strategy here is the same as the loops: separate work repeated from repetition then combine that information}

At this point we need to be clear about the mechanics of recursion. On one hand, it behaves no differently than any other procedure call, but on the other hand the procedure is calling itself and managing this sameness and otherness can take practice.  Let's say we start with a vector containing $\{3,1,7,5\}$ and we're searching for the key $2$. This is an example of the worst case where the search key will not be found. The first call to the recursive search has $fst=0$ and $lst=4$. This instance of search makes a call with $fst=1$ and $lst=4$, that instance make call with $fst=2$ and $lst=4$, and so on until finally the base case is reached where$fst=lst=4$. At this point each search returns -1 to the search in which it was called until finally the first instance returns -1 and the recursive chain is done as seen in figure \ref{fig:rectrace}.

\begin{figure*}[!htbp]
\begin{tabular}{ccccc}
search(data,2,0,4) & & & &  \\
$\hookrightarrow$ & search(data,2,1,4) & & & \\
& $\hookrightarrow$ & search(data,2,2,4) & & \\
&  &  $\hookrightarrow$ & search(data,2,3,4) & \\
& &  &  $\hookrightarrow$ & search(data,2,4,4) \\
& &  & search(data,2,3,4) & $\leftarrow$ -1  $\hookleftarrow$ \\
&  & search(data,2,2,4) & $\leftarrow$ -1  $\hookleftarrow$ & \\
& search(data,2,1,4) & $\leftarrow$ -1  $\hookleftarrow$ & & \\
search(data,2,0,4) &$\leftarrow$ -1  $\hookleftarrow$ & & & \\
$\leftarrow$ -1 $\hookleftarrow$ & & & & 
\end{tabular}
\caption{An example sequence of recursive search calls for $data=\{3,1,7,5\}$ }
\label{fig:rectrace}
\end{figure*}

The important thing to notice here is that for any value of $fst > lst$, search will call itself recursively while increasing $fst$ by one such that $fst$ will reach $lst$ eventually and cause the procedure to begin returning back to the original call to search. If each call adds one to $fst$ then $n$ calls adds $n$ and we need to determine when $fst+n=lst$ because at that point the recursion stops. It's clear that $n=lst-fst$. This is exactly the size of the region being searched. We now know that in the worst case, recursive search calls itself once for every element in the search region. For each of those calls a constant amount of work will be done. For a search region of size $n$\sidenote{again: $n=lst-fst$}, search will do a total of $O(n)*O(1)=O(n)$ work. So, to search the entire vector for the key we first do $O(1)$ work then call the recursive search which does linear in the size of the vector work. Recursive search is no more or less complex than iterative search. 

\subsection*{Recurrence Relations (OPTIONAL)}

To determine the complexity of the recursive search we analyzed the recursion in the same way that we analyzed loops: determine the work done per repetition, determine the amount of repetition, then combine the results as a product. There is a formal way of managing the analysis of recursive procedures using recursive mathematical functions. You'll learn this in Discrete Math and use it more advanced CS courses but I'll give you a quick taste of it here.

Let's say that $T$ is the time function of our search procedure such that for searching a vector region size $n=lst-fst$, $T(n)$ is the time complexity of search. The base case occurs when $n=0$ and in that case we find that search does a constant amount of work, or $T(0) = O(1)$. For $n>0$, we see that the procedure does a constant amount of work then makes a recursive call to search a vector region of size $n-1$. We express this with the \textsc{recurrence relation}
\[
T(n) = T(n-1) + O(1)
\]
Our goal now is to solve this equation in such a way as to remove the recursion of $T$ on the right hand side. One way to do this is simply to unwind the recursion in order to discover the pattern.
\begin{equation*}
\begin{array}{rcl}
T(n) &= & T(n-1) + O(1) \\
 &=& (T(n-2) + O(1)) + O(1) = T(n-2) + 2(O(1)) \\  
 &=& (T(n-3) + O(1)) + 2*O(1) = T(n-3) + 3(O(1)) \\
 &=& (T(n-4) + O(1)) + 3*O(1) = T(n-4) + 4(O(1)) \\
 &=& \ldots \\
 &=& T(n-k) + k*O(1)
\end{array}
\end{equation*}
At this point we see that after $k$ recursive calls we've accumulated $k$ units of constant work. When $k=n$ then we reach $T(0)$ and find that $T(n) = T(0)+O(n)=O(n)$ work is done. Many techniques exist for solving recurrence relations. The one demonstrated here is really just a more formalized version of the analysis that we did when analyzing recursive search.

\section{Insertion Sort is Quadratic Time}

Given that we're utilizing the same techniques to solve sort that we did to solve search, it should come as no surprise then that the analysis of sort follows the same path as that of search. The key difference will be that sort requires a non-trivial helper \textit{insert}. Just like we did when managing the recursive version of search, we'll avoid getting overwhelmed by analyzing \textit{insert} and \textit{sort} independently and then when that's done we'll combine what we know to get a complete picture of the complexity of insertion sort. 

\subsection{Iterative Insertion Sort}

\subsection{Recursive Insertion Sort}




\end{document}