\documentclass[]{tufte-handout}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[pdftex]{graphicx}

\title{COMP 161 - Lecture Notes - 16 - Complexity and Big-O}
\date{}

\newtheorem{define}{Definition}
\newtheorem{theorem}{Theorem}

\begin{document} 
\maketitle

\begin{abstract}
In these lecture notes we look at algorithm complexity and the use of Big-O notation to express complexity classes.
\end{abstract}

\section{Complexity}

Let's recall a few key points about the time complexity classification of a procedure:
\begin{itemize}
\item It's a measurement of elementary operations needed as a function of the size of the vector\sidenote{or some similar property of the input}. For an input with size $n$ we'll often denote this function as $\mathcal{T}(n)$. 
\item It's based on the worst case, or maximum number of operations, for the size. It's meant as an upper bound. 
\end{itemize}

Recall that we use complexity to capture the big picture of efficiency by classifying procedures\sidenote{really the underlying algorithm} into key categories. For example, if we're talking about motor vehicles, then calling something and SUV versus a Hybrid car implies something about the fuel consumption efficiency of the vehicle. We recognize that these two classes of vehicles perform very differently and that within each class there's more variability still. Computational complexity classes essentially do the same thing. They set aside the specific details of the procedure's performance to emphasize overall characteristics. What we want to do is equate the time complexity function for a procedure to a well understood function. By doing so we can reduce the infinite possible actual time  

\subsection{ Big-O }

There is a precise mathematical tool call \textsc{Big-O notation} that we can use to express the relationship we're looking for when expressing procedure complexity. 
\begin{define}
Let the functions $f$ and $g$ for positive integers.  Then the function $f$ is \textit{on the order of} $g$ if there exists constants $n_0 \geq 0 $ and $\alpha$ such that for all $n 	\geq n_0$ 
\[
f(n) \leq \alpha g(n)
\]
We write this as $f(n) = O(g(n))$, or sometimes just $f = O(g)$,\sidenote{O is uppercase letter o, not the number zero} and often say ``$f$ is Big-O of $g$'' as opposed to ``$f$ is on the order of $g$''.
\label{def:bigO}
\end{define}

Let's pick this apart so that we understand what saying $f = O(g)$ really means. First notice it's clearly an upperbound relationship. The value $\alpha g(n)$ is at least as big as $f(n)$. The $\alpha$ term is allowing us to focus not on the specific value of $g(n)$ as our bound but nearly any multiple of $g(n)$.  We're more or less saying that ``$f(n)$ is no larger than some multiple of $g(n)$''.  Finally, by throwing in $n_0$ we're emphasizing ``larger'' values of $n$ for whatever large means to us. Putting this all together we see that $f = O(g)$ really means that ``for all $n$ past some point\sidenote{$n_0$}, we can draw some multiple\sidenote{$\alpha$} of $g$ on or above $f$''.  The notion of Big-O gives us the right mixture of specificity and flexibility for capturing complexity and complexity classes.   

In future courses you'll explore this formalism in more depth. Right now we want to learn some basic rules and relationships that follow such that we understand how Big-O communicates complexity classes. There are three basic consequences of definition \ref{def:bigO} that are essential to working with Big-O in both a formal and informal manner. I'll state them formally and then provide some insight as to why they are so essential to complexity analysis.

The order of a sum is the sum of the orders. 
\begin{theorem}
Let $f$ and $g$ be functions over the positive integers. Then,
\[
O(f + g) = O(f) + O(g)
\]
\label{th:sum}
\end{theorem}
All procedures can be broken down into a series of steps. This theorem (\ref{th:sum}) tells us that it is enough to know the order of each step. This let's us greatly simplify complexity analysis by doing it piecewise rather than on the whole. 

We have two properties that deal with produces and Big-O. The order of a product is the product of the orders.
\begin{theorem}
Let $f$ and $g$ be functions over the positive integers. Then,
\[
O(fg) = O(f)O(g)
\]
\label{th:mult}
\end{theorem}
The order of a function is invariant under multiplication by a constant.
\begin{theorem}
Let $f$ and $g$ be functions over the positive integers. Then for all real-values $\alpha$,
\[
O(\alpha g) = O(g)
\]
\label{th:constmult}
\end{theorem}
A great number of procedures involve repetition and repetition induces a product.\sidenote{Repeating $k$ operators $t$ times takes $tk$ operations in total}. When the repetition is done a fixed number of times, then theorem (\ref{th:constmult}) tells us that the order of the repetition is the same as the the order of the thing being repeated. Similarly, if we repeat some fixed, constant amount of work, some function of our input size number of times, then theorem (\ref{th:constmult}) tells us that the order of the repetition is the order of them number of function that determines the number of repetitions. If, however, the repetition and the work are both functions of our input size, then theorem (\ref{th:mult}) tells us that we can at least analyze the order of the repetition and the order of the repeated work in parts but cannot really simplify to one order or the other. 


Using Big-O we can establish a strict ordering of some functions. This will be true of the functions on which we'll base our complexity classes. It turns out that larger orders dominate smaller orders. 
\begin{theorem}
For functions $f$ and $g$. If $O(f) < O(g)$, then
\[
O(f) + O(g) = O(g)
\]
\label{th:order}
\end{theorem}
Once again, sums arise from the discrete steps of a computation. When one step requires an order of magnitude more computation than another step, then that step dominates the total running time of the computation to the point that the whole computation behaves as if it were only of that larger complexity class. What theorem \ref{th:order} lets us do is focus solely on thethe critical step(s) that account for most of the work.\sidenote{I tend to think of this rule in terms of spending money. If you're paying thousands of dollars for something, then adding on tens of dollars doesn't really change the big picture-- you're paying thousands of dollars for something.If you're paying \$5000, then you'll probably also pay $5050$}. 

Hopefully you're picking up on the fact that these rules have close ties to the constructs of programs. The step-wise nature of induces sums in our time complexity functions and those sums can be more easily managed using theorem (\ref{th:sum}). Repetition induces products in our time complexity functions and those produces can be managed with theorem (\ref{th:mult}) and (\ref{th:constmult}). The analysis of conditionals is simplified by the fact that we're only concerned with the worst case. If we know which branch has the highest order, then we can focus on that branch. 

\section{Some Complexity Classes}

Remember our intended use for Big-O notation is the classification of the time complexity of procedures in order to address the questions like whether or not one procedure is more or less complex than another or if a given procedure is too complex to be usable in practice. A large majority of the procedures we encounter have a time function which is on the order of one of the following functions. These functions, in turn, establish a clear ordering of classes.

\begin{enumerate}
\item \textsc{Constant} \\
 If the complexity function $f$ is constant, i.e. $f(n) = c$ for some fixed value $c$,  then we say $f = O(1)$.

\item \textsc{Logarithmic} \\
 If the complexity function $f$ is  logarithmic, i.e. $f(n) = \log_b n$, then we say $f = O(\log n)$. Notice that we leave off the base of the logarithm. It can be shown that the base of the logarithm actually doesn't matter in Big-O notation\sidenote{$O(\log_a n) = O(\log_b n)$ for all $a$ and $b$} so we typically use 2. You'll often see $\log_2$ written as $lg$ 

\item \textsc{Linear} \\
 If the complexity function $f$ is linear, i.e. $f(n) = n$, then we say $f = O(n)$. 

\item \textsc{Linearithmic}\sidenote{a blend of linear and logarithmic} \\
 If the complexity function $f$ is the product of linear and logarithmic function, i.e. $f(n) = n\log n$, then we say $f = O(n\log n)$.

\item \textsc{Quadratic} \\
 If the complexity function $f$ is a quadratic function, i.e. $f(n) = n^2$, then we say $f = O(n^2)$.

\item \textsc{Cubic} \\ 
If the complexity function $f$ is a cubic function, i.e. $f(n) = n^3$, then we say $f = O(n^3)$.	

\item \textsc{Exponential} \\
If the complexity function $f$ is a exponential for some constant $c>1$, i.e. $f(n) = c^n$. We express this as  $f = O(c^n)$.

\item \textsc{Factorial} \\
If the complexity function $f$ is the factorial, i.e. $f(n) = n!$, then we say $f = O(n!)$.
\end{enumerate}

Within each class there is an awful lot of nuance. For example, pick any time function where $\mathcal{f}(n)= an + b$, then $f = O(n)$. As you may remember from math class, the larger the value of $a$ the greater the slope of the line and the faster it grows. Big-O notation completely ignores this reality and instead groups all lines together. That's ok, the point of Big-O and complexity classes are not specific, nuanced details, but big picture, order of magnitude details.

The complexity classes we use are represented by basic, well understood functions for which there is a strict least to greatest growth rate order.
\[
O(1) < O(\log n) < O(n) < O(n \log n) < O(n^2) < O(n^3) < O(c^n) < O(n!)
\]
If you have two different procedures that solve the same problem and one has work $O(n)$ and the other has work $O(n \log n)$, then you know the first, the linear work option, is \textit{a whole order of magnitude better} than the later. This is what we get from Big-O and complexity classes. We get the ability to partition the infinite world of procedures into simple efficiency complexity classes that allow us to quickly and easily identify order of magnitude differences.  

On the practical side of things, we can also quickly identify when a procedure will be impractical. Computer science has drawn a line in the sand at \textsc{Polynomial complexity} such that any complexity class that is a polynomial or better is theoretically efficient. This includes everything on our list but $O(2^n)$ and $O(n!)$. It turns out that this somewhat arbitrary choice works pretty well in practice. However, cubic and quadratic functions do grow pretty quickly and can easily be impractical for even modest $n$.  Superlinear or better is almost always practical, even for very large $n$.  You'll get a very real sense of this in your final project, but let's look at some numbers to get you thinking. 


First lets look at how each function grows as $n$ grows.
\begin{figure}
\includegraphics[scale=0.45]{nvals.png}
\caption{The value of key complexity class functions for increasing values of $n$}
\end{figure}

Now imagine you have a computer that performs 1 operation every nanoseconds\sidenote{also known as a 1 Gigaflop GFLOP computerThis is actually fairly modest by today's standards. Your average laptop can probably do two to five times better than this.} Then, we can attach some time amounts to these complexity classes.
\begin{figure}
\includegraphics[scale=0.45]{timevals.png}
\caption{The time needed to compute key complexity class functions at 1GFLOP}
\end{figure}





\end{document}